{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pickle import load\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(train_loss, val_loss):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(train_loss.index.tolist(), train_loss.tolist(),\n",
    "             lw=3, label='Train Loss')\n",
    "    plt.plot(val_loss.index.tolist(), val_loss.tolist(),\n",
    "             lw=3, label='Validation Loss')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('Loss', fontsize=20)\n",
    "    plt.title('Training and Validation Loss', fontsize=20)\n",
    "    plt.legend(loc='best', fontsize=16)\n",
    "    plt.grid()\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, loss_function, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_loss = 1e10\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    loss_history = {'train': [], 'val': []}\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print('\\nEpoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training phase and a validation phase at every 10 epochs\n",
    "        for phase in ['train', 'val']:\n",
    "            # Set model to training or evaluation mode\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, (inputs, labels) in tqdm(enumerate(dataloaders[phase]),\n",
    "                                              leave=True,\n",
    "                                              total=len(dataloaders[phase])):\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = loss_function(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "\n",
    "            # if phase == 'train':\n",
    "            #     scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            loss_history[phase].append(epoch_loss)\n",
    "            if epoch % 2 == 0:\n",
    "                if phase == 'train':\n",
    "                    train_stats = '{} ==> Loss: {:.4f}'.format(phase.upper(), epoch_loss)\n",
    "                else:\n",
    "                    # print(train_stats)\n",
    "                    print('\\n'+train_stats+' -- {} ==> Loss: {:.4f}'.format(phase.upper(), epoch_loss))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(model.state_dict(), 'models/LSTM_v0.pth')\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, loss_history\n",
    "\n",
    "\n",
    "class RUL_Estimator(nn.Module):\n",
    "\n",
    "    def __init__(self, n_features, hidden_dim=100, dropout=0.2, seq_length=40, num_layers=2, output_dim=1):\n",
    "        super(RUL_Estimator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_length = seq_length\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=self.hidden_dim * 2, out_features=output_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lstm_out, _ = self.lstm(input)\n",
    "        pred = F.relu(self.linear(lstm_out))\n",
    "        # [:, -1, :] as we are interested in the last element of the sequence\n",
    "        return pred[:, -1, :]\n",
    "\n",
    "\n",
    "class CMAPSSDataset(Dataset):\n",
    "    \"\"\"N-CMAPSS dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, sep=' ', seq_len=40):\n",
    "        \"\"\"\n",
    "        :param csv_file (string): Path to the csv dataset file.\n",
    "        \"\"\"\n",
    "        self.df_cmapss = pd.read_csv(csv_file, sep=sep)\n",
    "        self.df_data = self.df_cmapss.loc[:, 'unit':'phi']\n",
    "        # drop 'unit' and column 0\n",
    "        self.feature_columns = self.df_data.columns[1:]\n",
    "        self.targets = self.df_cmapss[['unit', 'RUL']]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.seq_gen = (list(self.gen_sequence(self.df_data[self.df_data['unit'] == id],\n",
    "                                               self.feature_columns))\n",
    "                        for id in self.df_data['unit'].unique() if\n",
    "                        len(self.df_data[self.df_data['unit'] == id]) >= seq_len)\n",
    "\n",
    "        self.seq_data = np.concatenate(list(self.seq_gen)).astype(np.float32)\n",
    "\n",
    "        self.targets_gen = [self.gen_targets(self.targets[self.targets['unit'] == id], ['RUL'])\n",
    "                            for id in self.targets['unit'].unique() if\n",
    "                            len(self.targets[self.targets['unit'] == id]) >= seq_len]\n",
    "\n",
    "        self.seq_targets = np.concatenate(self.targets_gen).astype(np.float32)\n",
    "\n",
    "    # Function to generate sequences of shape: (samples, time steps, features)\n",
    "    def gen_sequence(self, id_df, feature_columns):\n",
    "        \"\"\" Only consider sequences that meets the window-length, no padding is used. This means for testing\n",
    "        we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "        we can use shorter ones \"\"\"\n",
    "        data_array = id_df[feature_columns].values\n",
    "        num_elements = data_array.shape[0]\n",
    "        if (num_elements != self.seq_len):\n",
    "            for start, stop in zip(range(0, num_elements - self.seq_len), range(self.seq_len, num_elements)):\n",
    "                yield data_array[start:stop, :]\n",
    "        else:\n",
    "            yield data_array[:num_elements, :]\n",
    "\n",
    "    # Function to generate labels\n",
    "    def gen_targets(self, id_df, label):\n",
    "        data_array = id_df[label].values\n",
    "        num_elements = data_array.shape[0]\n",
    "        return data_array[self.seq_len:num_elements, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data) - (self.seq_len - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.seq_data[idx]\n",
    "        target = self.seq_targets[idx]\n",
    "\n",
    "        data = torch.tensor(data)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 6.67 GiB for an array with shape (699820, 40, 32) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-7ee0a9eae073>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m cmapss_dataset = {x: CMAPSSDataset(csv_file='data/N-CMAPSS/'+x+'_DS03.csv',\n\u001B[1;32m      5\u001B[0m                                    sep=' ', seq_len=sequence_length)\n\u001B[0;32m----> 6\u001B[0;31m                   for x in ['train', 'val']}\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;31m# dataloaders = {x: DataLoader(cmapss_dataset[x], batch_size=batch_size,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-8-7ee0a9eae073>\u001B[0m in \u001B[0;36m<dictcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      4\u001B[0m cmapss_dataset = {x: CMAPSSDataset(csv_file='data/N-CMAPSS/'+x+'_DS03.csv',\n\u001B[1;32m      5\u001B[0m                                    sep=' ', seq_len=sequence_length)\n\u001B[0;32m----> 6\u001B[0;31m                   for x in ['train', 'val']}\n\u001B[0m\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;31m# dataloaders = {x: DataLoader(cmapss_dataset[x], batch_size=batch_size,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-14304de842e3>\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, csv_file, sep, seq_len)\u001B[0m\n\u001B[1;32m    126\u001B[0m                         len(self.df_data[self.df_data['unit'] == id]) >= seq_len)\n\u001B[1;32m    127\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseq_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mseq_gen\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfloat32\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m         self.targets_gen = [self.gen_targets(self.targets[self.targets['unit'] == id], ['RUL'])\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36mconcatenate\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;31mMemoryError\u001B[0m: Unable to allocate 6.67 GiB for an array with shape (699820, 40, 32) and data type float64"
     ]
    }
   ],
   "source": [
    "# batch_size = 2944\n",
    "batch_size = 1024\n",
    "sequence_length = 40\n",
    "cmapss_dataset = {x: CMAPSSDataset(csv_file='data/N-CMAPSS/'+x+'_DS03.csv',\n",
    "                                   sep=' ', seq_len=sequence_length)\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "# dataloaders = {x: DataLoader(cmapss_dataset[x], batch_size=batch_size,\n",
    "#                              num_workers=0, pin_memory=True, collate_fn=collate_batch)\n",
    "#                for x in ['train', 'test']}\n",
    "\n",
    "dataloaders = {x: DataLoader(cmapss_dataset[x], batch_size=batch_size,\n",
    "                             num_workers=0, pin_memory=True, shuffle=True)\n",
    "               for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in dataloaders['train']:\n",
    "    print(data.shape)\n",
    "    print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in dataloaders['val']:\n",
    "    print(data.shape)\n",
    "    print(labels.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}