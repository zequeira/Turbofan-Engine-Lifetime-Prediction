{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pickle import load\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_history(train_loss, val_loss):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.plot(train_loss.index.tolist(), train_loss.tolist(),\n",
    "             lw=3, label='Train Loss')\n",
    "    plt.plot(val_loss.index.tolist(), val_loss.tolist(),\n",
    "             lw=3, label='Validation Loss')\n",
    "    plt.xlabel('Epochs', fontsize=20)\n",
    "    plt.ylabel('Loss', fontsize=20)\n",
    "    plt.title('Training and Validation Loss', fontsize=20)\n",
    "    plt.legend(loc='best', fontsize=16)\n",
    "    plt.grid()\n",
    "    plt.savefig('loss_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class CMAPSSDataset(Dataset):\n",
    "    \"\"\"N-CMAPSS dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, sep=' ', seq_len=40):\n",
    "        \"\"\"\n",
    "        :param csv_file (string): Path to the csv dataset file.\n",
    "        \"\"\"\n",
    "        self.df_cmapss = pd.read_csv(csv_file, sep=sep)\n",
    "        self.df_data = self.df_cmapss.loc[:, 'unit':'phi']\n",
    "        # drop 'unit' and column 0\n",
    "        self.feature_columns = self.df_data.columns[1:]\n",
    "        self.targets = self.df_cmapss[['unit', 'RUL']]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.seq_gen = (list(self.gen_sequence(self.df_data[self.df_data['unit'] == id],\n",
    "                                               self.feature_columns))\n",
    "                        for id in self.df_data['unit'].unique() if\n",
    "                        len(self.df_data[self.df_data['unit'] == id]) >= seq_len)\n",
    "\n",
    "        self.seq_data = np.concatenate(list(self.seq_gen)).astype(np.float32)\n",
    "\n",
    "        self.targets_gen = [self.gen_targets(self.targets[self.targets['unit'] == id], ['RUL'])\n",
    "                            for id in self.targets['unit'].unique() if\n",
    "                            len(self.targets[self.targets['unit'] == id]) >= seq_len]\n",
    "\n",
    "        self.seq_targets = np.concatenate(self.targets_gen).astype(np.float32)\n",
    "\n",
    "    # Function to generate sequences of shape: (samples, time steps, features)\n",
    "    def gen_sequence(self, id_df, feature_columns):\n",
    "        \"\"\" Only consider sequences that meets the window-length, no padding is used. This means for testing\n",
    "        we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "        we can use shorter ones \"\"\"\n",
    "        data_array = id_df[feature_columns].values\n",
    "        num_elements = data_array.shape[0]\n",
    "        if (num_elements != self.seq_len):\n",
    "            for start, stop in zip(range(0, num_elements - self.seq_len), range(self.seq_len, num_elements)):\n",
    "                yield data_array[start:stop, :]\n",
    "        else:\n",
    "            yield data_array[:num_elements, :]\n",
    "\n",
    "    # Function to generate labels\n",
    "    def gen_targets(self, id_df, label):\n",
    "        data_array = id_df[label].values\n",
    "        num_elements = data_array.shape[0]\n",
    "        return data_array[self.seq_len:num_elements, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seq_data) - (self.seq_len - 1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.seq_data[idx]\n",
    "        target = self.seq_targets[idx]\n",
    "\n",
    "        data = torch.tensor(data)\n",
    "        target = torch.tensor(target)\n",
    "\n",
    "        return data, target\n",
    "\n",
    "\n",
    "class CMAPSSDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_data, val_data, test_data, seq_len=1,\n",
    "                 batch_size=1024, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.train_dataset = None\n",
    "        self.val_data = val_data\n",
    "        self.val_dataset = None\n",
    "        self.test_data = test_data\n",
    "        self.test_dataset = None\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    # def setup(self, stage=None):\n",
    "    #     if stage in (None, \"fit\"):\n",
    "    #         self.train_dataset = CMAPSSDataset(csv_file=self.train_data, sep=' ',\n",
    "    #                                            seq_len=self.seq_len)\n",
    "    #         self.val_dataset = CMAPSSDataset(csv_file=self.val_data, sep=' ',\n",
    "    #                                          seq_len=self.seq_len)\n",
    "    #\n",
    "    #     if stage in (None, \"test\"):\n",
    "    #         self.test_dataset = CMAPSSDataset(csv_file=self.test_data, sep=' ',\n",
    "    #                                           seq_len=self.seq_len)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CMAPSSDataset(csv_file=self.train_data, sep=' ',\n",
    "                                           seq_len=self.seq_len)\n",
    "        self.val_dataset = CMAPSSDataset(csv_file=self.val_data, sep=' ',\n",
    "                                         seq_len=self.seq_len)\n",
    "        self.test_dataset = CMAPSSDataset(csv_file=self.test_data, sep=' ',\n",
    "                                          seq_len=self.seq_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "\n",
    "class LSTMRul(pl.LightningModule):\n",
    "    def __init__(self, n_features, hidden_dim=50, dropout=0.2, seq_len=40, num_layers=2,\n",
    "                 output_dim=1, criterion=None, learning_rate=1e-3):\n",
    "        super(LSTMRul, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Define the LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.linear = nn.Linear(in_features=hidden_dim * 2, out_features=output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        pred = torch.relu(self.linear(lstm_out))\n",
    "        return pred[:, -1, :]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        # self.log('train_loss', loss, prog_bar=True, logger=True)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log('val_loss', loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n",
    "        # self.log('val_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        # self.log('test_loss', loss, prog_bar=True, logger=True)\n",
    "        self.log('test_loss', loss, prog_bar=True, on_step=False, on_epoch=True, logger=True)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 6144\n",
    "batch_size = 64512\n",
    "sequence_length = 40\n",
    "EPOCHS = 10\n",
    "seed_everything(42, workers=True)\n",
    "\n",
    "data_module = CMAPSSDataModule(train_data='data/N-CMAPSS/train_DS03.csv',\n",
    "                               val_data='data/N-CMAPSS/val_DS03.csv',\n",
    "                               test_data='data/N-CMAPSS/test_DS03.csv',\n",
    "                               seq_len=sequence_length,\n",
    "                               batch_size=batch_size,\n",
    "                               num_workers=32)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMRul(\n",
      "  (criterion): MSELoss()\n",
      "  (lstm): LSTM(32, 100, num_layers=5, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (linear): Linear(in_features=200, out_features=1, bias=True)\n",
      ")\n",
      "1,073,801 total number of parameters\n",
      "1,073,801 parameters to train\n"
     ]
    }
   ],
   "source": [
    "model_params = dict(\n",
    "    n_features=32,\n",
    "    hidden_dim=100,\n",
    "    seq_len=sequence_length,\n",
    "    num_layers=5,\n",
    "    dropout=0.5,\n",
    "    output_dim=1,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "model = LSTMRul(**model_params)\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total number of parameters')\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} parameters to train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "  | Name      | Type    | Params\n",
      "--------------------------------------\n",
      "0 | criterion | MSELoss | 0     \n",
      "1 | lstm      | LSTM    | 1.1 M \n",
      "2 | linear    | Linear  | 201   \n",
      "--------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.295     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef44132f78745b1a7677d6cc095305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 86: val_loss reached 0.17948 (best 0.17948), saving model to \"/home/ec2-user/SageMaker/checkpoints_v2/LSTM-epoch=00-val_loss=0.18.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 173: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 260: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 347: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 434: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 521: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 608: val_loss reached 0.17417 (best 0.17417), saving model to \"/home/ec2-user/SageMaker/checkpoints_v2/LSTM-epoch=06-val_loss=0.17.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 695: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 782: val_loss was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 869: val_loss was not in top 1\n"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"checkpoints_v2\",\n",
    "    filename=\"LSTM-{epoch:02d}-{val_loss:.2f}\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "trainer = Trainer(\n",
    "    deterministic=True,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    max_epochs=EPOCHS,\n",
    "    gpus=8,\n",
    "    strategy='dp'\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/pytorch_lightning/core/datamodule.py:470: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  f\"DataModule.{name} has already been called, so it will not be called again. \"\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b592d53ca0b42ff9194c668e3f7b26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.20492902398109436}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.20492902398109436}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Start tensorboard.\n",
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir lightning_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTMRul(\n",
       "  (criterion): MSELoss()\n",
       "  (lstm): LSTM(32, 100, num_layers=5, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=200, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'checkpoints_v2/LSTM-epoch=06-val_loss=0.17.ckpt'\n",
    "model = LSTMRul.load_from_checkpoint(PATH, **model_params)\n",
    "\n",
    "print(model.learning_rate)\n",
    "# prints the learning_rate you used in this checkpoint\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.24.2 when using version 0.24.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# load the scaler\n",
    "target_scaler = load(open('data/N-CMAPSS/target_scaler_DS03.pkl', 'rb'))\n",
    "\n",
    "RMSE = []\n",
    "with torch.no_grad():\n",
    "    for test_data, test_labels in data_module.test_dataloader():\n",
    "        test_labels = target_scaler.inverse_transform(test_labels)\n",
    "        pred = model(test_data)\n",
    "        pred = target_scaler.inverse_transform(pred.cpu())\n",
    "        RMSE.append(mean_squared_error(test_labels, pred, squared=False))\n",
    "    print(f'Test RMSE: {np.mean(RMSE)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
